{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c030218b-dec9-4e06-a127-dd91d83c1d9a",
   "metadata": {},
   "source": [
    "# Split UKBB Data\n",
    "I downloaded scans from 1000 subjects of the UKBB dataset. We have manual annotations for 31 of them.\n",
    "I split the data as follwing:\n",
    "- Training set:  925 subjects\n",
    "- Validation set: 30 subjects\n",
    "- Test set: 31 patients\n",
    "- Exclude: 14 patients (More than 5 sections exist for some subjects and Im not sure why, other subject were badly segmentated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622ffb18-ac90-4c8e-a208-631b316903dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    EnsureChannelFirst,\n",
    "    Spacing,\n",
    "    Lambda,\n",
    ")\n",
    "\n",
    "# private libraries\n",
    "import sys\n",
    "\n",
    "if \"../scripts\" not in sys.path:\n",
    "    sys.path.insert(1, \"../scripts\")\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7f22c1-3224-4ef1-a03f-08d82f7001b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 14 patients.\n"
     ]
    }
   ],
   "source": [
    "# Load manifest\n",
    "data = pd.read_csv(config.ukbb + \"manifest.csv\")\n",
    "\n",
    "# Exlude patients\n",
    "# Lina said segmentation inst working well for these samples\n",
    "bad_samples = [\n",
    "    int(s)\n",
    "    for s in [\n",
    "        \"1013238\",\n",
    "        \"1013386\",\n",
    "        \"1017226\",\n",
    "        \"1017752\",\n",
    "        \"1018450\",\n",
    "        \"1018829\",\n",
    "        \"1010497\",\n",
    "        \"1010973\",\n",
    "        \"1008327\",\n",
    "    ]\n",
    "]\n",
    "patients2remove = data.loc[data[\"section\"] == 6][\"eid\"].unique()\n",
    "patients2remove = list(patients2remove) + bad_samples\n",
    "_index = data.loc[data[\"eid\"].apply(lambda x: x in patients2remove)].index\n",
    "data = data.drop(_index).reset_index(drop=True)\n",
    "print(f\"Dropped {len(patients2remove)} patients.\")\n",
    "\n",
    "# Focus on sections 1, 2 and 3\n",
    "data = data.loc[data[\"section\"].apply(lambda x: x in [1, 2, 3])]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Map water only labels to other sequence types\n",
    "data[\"label\"] = data[\"image\"].apply(lambda x: x.replace(\"/\", \"_\"))\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: x.replace(\"in\", \"W\"))\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: x.replace(\"opp\", \"W\"))\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: x.replace(\"F\", \"W\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1454b-2f06-4d1b-a0c6-8e44968b4f87",
   "metadata": {},
   "source": [
    "## Choose good segmentations based on segmentation volume\n",
    "The Dice score of segmentations correlates stronlgy to their segmentation volume. Therefore, I limit train and validation set to segmentations with increased volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bd1846-7a2e-45cc-b32c-1cfff5f11c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Volume (takes around five minutes for 16 cores)\n",
    "calc_volume = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True),\n",
    "        EnsureChannelFirst(),\n",
    "        Spacing(pixdim=[3, 3, 3]),\n",
    "        Lambda(func=lambda x: (x != 0).sum()),\n",
    "        Lambda(func=lambda x: x.item()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate volume for water-only sequences.\n",
    "# Other sequence types will have same volume as they were derived from water only\n",
    "data[\"seg_volume\"] = 0\n",
    "_volume = list(\n",
    "    data.loc[data[\"dixon_type\"] == \"W\"][\"label\"].apply(\n",
    "        lambda x: calc_volume(config.ukbb + \"preds_combined/\" + x)\n",
    "    )\n",
    ")\n",
    "for seq in [\"in\", \"opp\", \"W\", \"F\"]:\n",
    "    data.loc[data[\"dixon_type\"] == seq, \"seg_volume\"] = _volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de892734-f70b-4cd6-b86d-3d0d1544bc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_quantile_thr(volumes, quantile=0.7):\n",
    "    \"\"\"Return threshold that is smaller than 'quantile' of the values in the input list.\"\"\"\n",
    "    volumes = list(volumes).copy()\n",
    "    volumes.sort()\n",
    "    return volumes[int(len(volumes) * (1 - quantile))]\n",
    "\n",
    "\n",
    "# calculate threshold\n",
    "thr_sec1 = calc_quantile_thr(data.loc[data[\"section\"] == 1][\"seg_volume\"])\n",
    "thr_sec2 = calc_quantile_thr(data.loc[data[\"section\"] == 2][\"seg_volume\"])\n",
    "thr_sec3 = calc_quantile_thr(data.loc[data[\"section\"] == 3][\"seg_volume\"])\n",
    "\n",
    "# select scans with high segmentation volume\n",
    "high_volume = pd.concat(\n",
    "    [\n",
    "        data.loc[(data[\"section\"] == 1) & (data[\"seg_volume\"] > thr_sec1)],\n",
    "        data.loc[(data[\"section\"] == 2) & (data[\"seg_volume\"] > thr_sec2)],\n",
    "        data.loc[(data[\"section\"] == 3) & (data[\"seg_volume\"] > thr_sec3)],\n",
    "    ]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d28281-841d-41c1-8528-00d1615f3772",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Split on subject level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d2a8a85-43c0-4bde-92be-725b60e75a08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found manual annotations for 31 patients.\n"
     ]
    }
   ],
   "source": [
    "# Patient level split\n",
    "patients = set(data[\"eid\"].unique())\n",
    "\n",
    "# Manual annotations\n",
    "annotations = [f.name for f in os.scandir(config.ukbb + \"annotations\") if f.name[-7:] == \".nii.gz\"]\n",
    "test_patients = set([int(f.split(\"_\")[0]) for f in annotations])\n",
    "for p in test_patients:\n",
    "    assert p in patients, \" Annoation incorrectly included\"\n",
    "print(f\"Found manual annotations for {len(test_patients)} patients.\")\n",
    "\n",
    "# Train and validation split\n",
    "random.seed(42)\n",
    "patients_remaining = patients.difference(test_patients)\n",
    "valid_patients = set(random.sample(list(patients_remaining), 30))\n",
    "train_patients = patients_remaining.difference(valid_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564b6b07-e60b-424f-b968-c9730da4e9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scans in section 1: 2580\n",
      " Scans in section 2: 2576\n",
      " Scans in section 3: 2596\n",
      " Scans in section 1: 96\n",
      " Scans in section 2: 96\n",
      " Scans in section 3: 92\n",
      " Scans in section 1: 124\n",
      " Scans in section 2: 124\n",
      " Scans in section 3: 124\n"
     ]
    }
   ],
   "source": [
    "train_set = high_volume.loc[high_volume[\"eid\"].apply(lambda x: x in train_patients)].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "valid_set = high_volume.loc[high_volume[\"eid\"].apply(lambda x: x in valid_patients)].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "test_set = data.loc[data[\"eid\"].apply(lambda x: x in test_patients)].reset_index(drop=True)\n",
    "\n",
    "for _set in [train_set, valid_set, test_set]:\n",
    "    for p in [1, 2, 3]:\n",
    "        print(f\" Scans in section {p}:\", sum(_set[\"section\"] == p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0fc02c-7e1b-4251-a92d-eb376babd6bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8914ad8f-012c-44de-bc8b-dc462c151515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set.to_csv(config.ukbb + \"test.csv\", index=False)\n",
    "valid_set.to_csv(config.ukbb + \"valid.csv\", index=False)\n",
    "train_set.sample(random_state=13, frac=1).to_csv(config.ukbb + \"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608914a6-82e9-4780-b78a-9e4e4acbefd4",
   "metadata": {},
   "source": [
    "## Save water-only data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db95d48-3223-4e8c-a603-f5121310feff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_w = lambda df: df.loc[df[\"dixon_type\"] == \"W\"].reset_index(drop=True)\n",
    "\n",
    "select_w(test_set).to_csv(config.ukbb + \"test_w_only.csv\", index=False)\n",
    "select_w(valid_set).to_csv(config.ukbb + \"valid_w_only.csv\", index=False)\n",
    "select_w(train_set).sample(random_state=13, frac=1).to_csv(\n",
    "    config.ukbb + \"train_w_only.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
